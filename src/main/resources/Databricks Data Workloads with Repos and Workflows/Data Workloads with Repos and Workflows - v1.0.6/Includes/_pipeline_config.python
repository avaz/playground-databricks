{"version":"NotebookV1","origId":2139271331630476,"name":"_pipeline_config","language":"python","commands":[{"version":"CommandV1","origId":2139271331630477,"guid":"8d633390-e130-4cf2-bdd1-133cc3a0efd1","subtype":"command","commandType":"auto","position":2.0,"command":"from dbacademy import dbgems\n\nclass PipelineConfig:\n    \"\"\"\n    Represents pipeline settings with option to provide as JSON definition.\n\n      Attributes:\n          Configurable: name, storage, target, configuration, notebooks, policy\n          Auto-generated: libraries (from notebooks), clusters (from policy)\n          Non-configurable: continuous (False), development (True), photon (False)\n\n      Methods:\n          get_pipeline_settings(convert_to_json=False): \n              Returns pipeline settings as dictonary or JSON string\n\n          print_pipeline_config(): \n              Displays parameters as text and input textboxes\n\n    \"\"\"\n\n    def __init__(self, name, notebooks, configuration, storage, target, policy=None, photon=False):\n        \"\"\"\n        Defines PipelineConfig attributes.    \n\n            Use input values to set attributes: name, storage, target\n\n            Modify input values to set attributes:\n                configuration: add \"spark.master\": \"local[*]\" to configuration\n                notebooks: convert to list of notebook paths\n                libraries: create list of dictionaries from notebook paths\n                clusters: create based on policy\n            \n            Use default values to set attributes:\n                continuous: False\n                development: True\n                photon: False\n        \"\"\"\n        self.name = name\n        self.storage = storage\n        self.target = target\n        self.configuration = {**configuration, \"spark.master\": \"local[*]\"}\n\n        basepath = dbgems.get_notebook_dir()\n\n        self.notebooks = [f\"{basepath}/{n}\" for n in notebooks]\n        self.libraries = [{\"notebook\": {\"path\": n}} for n in self.notebooks]\n        self.policy = policy\n    \n        if policy:\n            clusters = [{\"num_workers\": 0, \"policy_id\": policy.get(\"policy_id\")}]\n        else:\n            clusters = [{\"num_workers\": 0}]\n        self.clusters = clusters\n            \n        self.continuous = False\n        self.development = True\n        self.photon = photon\n\n\n    def get_pipeline_settings(self, convert_to_json=False):\n        \"\"\"\n        Returns pipeline settings as dictonary or JSON string.\n\n        :param convert_to_json: if True, return JSON string; if False (default), return dictionary\n        :return: pipeline settings as a dictionary or JSON string, based on convert_to_json\n        \"\"\"\n        params = dict()\n        params[\"name\"] = self.name\n        params[\"target\"] = self.target\n        params[\"storage\"] = self.storage\n        params[\"libraries\"] = self.libraries    \n        params[\"configuration\"] = self.configuration\n        params[\"clusters\"] = self.clusters        \n        params[\"continuous\"] = self.continuous\n        params[\"development\"] = self.development\n        params[\"photon\"] = self.photon\n\n        if not convert_to_json:\n            return params\n        else:\n            import json\n            return json.dumps(self.get_pipeline_settings())\n\n\n    def get_config_values(self):\n        \"\"\"\n        Returns a subset of pipeline parameters as a list of (name, value) tuples.\n        This is meant to be displayed using DBAcademyHelper.display_config_values to provide instructions.\n\n        Included settings:\n            Pipeline Name: <name>\n            Storage Location: <path>\n            Target: <name>\n            Policy: <name>\n\n            All configuration properties, excluding \"spark.master\"; usually includes \"source\"\n                source: <path>\n\n            All notebook paths, numbered starting from 1:\n                Notebook #1 Path: <path>\n                Notebook #2 Path: <path>\n\n        See also DBAcademyHelper.display_config_values\n        \"\"\"\n\n        config_values = [\n            (\"Pipeline Name\", self.name),\n            (\"Storage Location\", self.storage),\n            (\"Target\", self.target),            \n            (\"Policy\", self.policy.get(\"name\"))\n        ]\n        for key, val in self.configuration.items():\n            if key != \"spark.master\": config_values.append((key, val))\n            \n        for i, path in enumerate(self.notebooks):\n            config_values.append((f\"Notebook #{i+1} Path\", path))\n            \n        return config_values\n\nNone        ","commandVersion":1,"state":"finished","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"5d16d6dd-44c6-405a-bc32-306478883a9f"},{"version":"CommandV1","origId":2139271331630478,"guid":"7b0a0c2c-3f58-4bb4-9ef9-946a190e1b7f","subtype":"command","commandType":"auto","position":3.0,"command":"# Define helper functions to configure, create, and trigger pipelines with DBAcademyHelper\n\n@DBAcademyHelper.monkey_patch\ndef configure_pipeline(self, notebooks, name=None, source=None, configuration=None, photon=False):\n    \"\"\"\n    Creates PipelineConfig object for provided notebooks and optional settings.\n    Defines parameter values using DBAcademyHelper:\n        Attributes:\n            target\n            paths.storage\n            pipeline_name (if name not provided)\n            source (if source not provided)\n        Methods:\n            get_dlt_policy()\n    :param notebooks (list): list of notebook paths, relative to current notebook\n    :param source (str): value for \"source\" configuration property\n    :param configuration (dict): overrides source to include more than one \"source\" configuration property (optional)\n    :param name (str): overrides self.pipeline_name to create pipeline name\n    \"\"\"\n    \n    notebooks = [f\"Pipeline/{f}\" for f in notebooks]\n    \n    if name is None:\n        name = self.pipeline_name\n\n    if configuration:\n        assert source is None, \"source is ignored when configuration is provided\"\n    elif source:\n        configuration = {\"source\": source}\n    else:\n        configuration = {\"source\": self.paths.stream_source}\n    \n\n    self.pipeline_config = PipelineConfig(\n        name=name,\n        notebooks=notebooks,        \n        configuration=configuration,\n        storage=self.paths.storage_location,\n        target=self.schema_name,\n        policy=self.get_dlt_policy(),\n        photon=photon\n    )\n\n@DBAcademyHelper.monkey_patch\ndef get_dlt_policy(self):\n    \"\"\"\n    Returns cluster policy for DLT pipelines, created by Workspace-Setup script.\n    Get cluster policy using name provided by DBAcademy ClustersHelper.POLICY_DLT_ONLY\n    \"\"\"\n    from dbacademy import common\n    from dbacademy.dbhelper import ClustersHelper\n    dlt_policy = self.client.cluster_policies.get_by_name(ClustersHelper.POLICY_DLT_ONLY)\n    if dlt_policy is None: \n        common.print_warning(\"WARNING: Policy Not Found\", \n        f\"Could not find the cluster policy \\\"{ClustersHelper.POLICY_DLT_ONLY}\\\".\\nPlease run the notebook Includes/Workspace-Setup to address this error.\")\n    return dlt_policy\n\n\n@DBAcademyHelper.monkey_patch\ndef generate_pipeline(self, config=None, show_validate=False):\n    \"\"\"\n    Creates pipeline using provided PipelineConfig or DBAcademyHelper.pipeline_config.\n\n    See also DBAcademyHelper.create_pipeline\n    See also DBAcademyHelper.validate_pipeline_config\n    See also PipelineConfig.print_pipeline_config\n\n    :param config: overrides DBAcademyHelper.pipeline_config (optional)\n    :param show_validate: if False (default), hide validation results\n    :return: pipeline ID\n    \"\"\"\n    if config is None: config = self.pipeline_config\n     \n    self.display_config_values(config.get_config_values())\n\n    self.pipeline_id = self.create_pipeline(config)\n\n    if show_validate: self.validate_pipeline_config(config, display=False)    \n\n    return self.pipeline_id\n\n\n\n@DBAcademyHelper.monkey_patch\ndef create_pipeline(self, config=None):\n    \"\"\"\n    Create or replace pipeline with the provided configuration, then display link to Pipeline UI.\n    See also DBAcademyHelper.create_pipeline_from_settings.\n\n    Sets DBAcademyHelper attribute:\n        pipeline_id: Pipeline ID of the created pipeline\n\n    :param config: PipelineConfig to override self.pipeline_config (optional)\n    :return: the pipeline ID of the created pipeline\n    \"\"\"\n    if not config: \n        config = self.pipeline_config\n    settings = config.get_pipeline_settings()\n\n    self.pipeline_id = self.create_pipeline_from_settings(settings)\n    \n    pipeline_url = f\"{dbgems.get_workspace_url()}#joblist/pipelines/{self.pipeline_id}\"\n    displayHTML(f\"\"\"Created the pipeline \"{settings[\"name\"]}\": <a href={pipeline_url}>{self.pipeline_id}</a>\"\"\")\n\n    return self.pipeline_id\n        \n\n@DBAcademyHelper.monkey_patch\ndef create_pipeline_from_settings(self, settings):\n    \"\"\"\n    Create or replace pipeline with the provided configuration, then return pipeline ID.\n    See also DBAcademyHelper.client.pipelines.\n\n    :param settings: dictionary of pipeline settings\n    :return: pipeline ID of the created pipeline\n    \"\"\"\n    self.client.pipelines().delete_by_name(settings[\"name\"]) \n    response = self.client.pipelines().create(**settings)\n\n    return response.get(\"pipeline_id\")\n\n\n@DBAcademyHelper.monkey_patch\ndef start_pipeline(self, pipeline_id=None, blocking=True):\n    \"\"\"\n    Starts the pipeline and then blocks until it has completed, failed or was canceled\n    :param pipeline_id: overrides self.pipeline_id to identify pipeline (optional)\n    \"\"\"\n    import time\n    from dbacademy.dbrest import DBAcademyRestClient\n\n    if not pipeline_id: pipeline_id = self.pipeline_id\n    \n    client = DBAcademyRestClient() \n    start = client.pipelines().start_by_id(pipeline_id)  # start pipeline\n    update_id = start.get(\"update_id\")\n\n    # get status and block until done\n    update = client.pipelines().get_update_by_id(pipeline_id, update_id)\n    state = update.get(\"update\").get(\"state\")\n\n    if blocking:\n      while state not in [\"COMPLETED\", \"FAILED\", \"CANCELED\"]:\n          duration = 15\n          time.sleep(duration)\n          print(f\"Current state is {state}, sleeping {duration} seconds.\")    \n          update = client.pipelines().get_update_by_id(pipeline_id, update_id)\n          state = update.get(\"update\").get(\"state\")\n\n      print(f\"The final state is {state}.\")\n\n      assert state == \"COMPLETED\", f\"Expected the state to be COMPLETED, found {state}\"\n\n    else:\n      print(f\"The current state is {state}.\")\n      \n    return update_id\n\n\n@DBAcademyHelper.monkey_patch\ndef validate_pipeline_config(self, config, display=True):\n    \"\"\"\n    Validate the configuration of the pipeline.\n    \n    - validate pipeline with name exists\n    - validate settings for storage, target\n    - validate libraries has correct count, includes each notebook\n    - validate configuration parameters for source, spark.master\n    - validate cluster settings: cluster count, autoscaling disabled, cluster policy, worker count\n    - validate settings for development mode, current channel, pipeline triggered mode\n\n    :param config: PipelineConfig to identify and validate pipeline\n    :param display: if True, displays validation results in a table\n    \"\"\"\n    from dbacademy import common\n    from dbacademy.dbhelper import ClustersHelper\n    suite = self.tests.new(\"Pipeline Config\")\n\n    # validate pipeline with name exists        \n    pipeline = self.client.pipelines().get_by_name(config.name)        \n    suite.test_not_none(lambda: pipeline, \n                        description=f\"Create the pipeline \\\"<b>{config.name}</b>\\\".\", \n                        hint=\"Double check the spelling.\")\n    if pipeline is None: pipeline = {}\n    spec = pipeline.get(\"spec\", {})\n\n    # validate storage location and target\n    suite.test_equals(lambda: spec.get(\"storage\", None), config.storage, \n                      description=f\"Set the storage location to \\\"<b>{config.storage}</b>\\\".\", \n                      hint=f\"Found \\\"<b>[[ACTUAL_VALUE]]</b>\\\".\")\n    suite.test_equals(lambda: spec.get(\"target\", None), config.target, \n                      description=f\"Set the target to \\\"<b>{config.target}</b>\\\".\", \n                      hint=f\"Found \\\"<b>[[ACTUAL_VALUE]]</b>\\\".\")\n\n    # validate notebooks\n    libraries = [l.get(\"notebook\", {}).get(\"path\") for l in spec.get(\"libraries\", [])]\n    config_libraries = [l.get(\"notebook\", {}).get(\"path\") for l in config.libraries]\n    def test_notebooks():\n        if libraries is None: return False\n        if len(libraries) != len(config.notebooks): return False\n        for library in libraries:\n            if library not in config_libraries: return False\n        return True\n    hint = f\"\"\"Found the following {len(libraries)} notebook(s):<ul style=\"margin-top:0\">\"\"\"\n    for library in libraries: hint += f\"\"\"<li>{library}</li>\"\"\"\n    hint += \"</ul>\"\n    suite.test(test_function=test_notebooks, actual_value=libraries, description=\"Configure the Notebook library.\", hint=hint)\n\n    # validate configuration parameters: source, spark.master\n    suite.test_equals(lambda: spec.get(\"configuration\", {}).get(\"source\"), config.configuration.get(\"source\"), \n                      description=f\"Set the \\\"<b>source</b>\\\" configuration parameter to \\\"<b>{config.configuration.get('source')}</b>\\\".\", \n                      hint=f\"Found \\\"<b>[[ACTUAL_VALUE]]</b>\\\".\")\n    suite.test_equals(lambda: spec.get(\"configuration\", {}).get(\"spark.master\"), \"local[*]\", \n                      description=f\"Set the \\\"<b>spark.master</b>\\\" configuration parameter to \\\"<b>local[*]</b>\\\".\", \n                      hint=f\"Found \\\"<b>[[ACTUAL_VALUE]]</b>\\\".\")\n    # suite.test_length(lambda: spec.get(\"configuration\", {}), 2, description=f\"Set the two configuration parameters.\", hint=f\"Found [[LEN_ACTUAL_VALUE]] configuration parameter(s).\")                      \n\n    # validate cluster settings: cluster count, autoscaling disabled, cluster policy, worker count\n    suite.test_length(lambda: spec.get(\"clusters\"), expected_length=1, \n                      description=f\"Expected one and only one cluster definition.\", \n                      hint=\"Edit the config via the JSON interface to remove the second+ cluster definitions\")\n    suite.test_is_none(lambda: spec.get(\"clusters\")[0].get(\"autoscale\"), description=f\"Autoscaling should be disabled.\")\n\n    def test_cluster_policy():\n        cluster = spec.get(\"clusters\")[0]\n        policy_id = cluster.get(\"policy_id\")\n        if policy_id is None: common.print_warning(\"WARNING: Policy Not Set\", \n                                                   f\"Expected the policy to be set to \\\"{ClustersHelper.POLICY_DLT_ONLY}\\\".\")\n        else:\n            policy_name = self.client.cluster_policies.get_by_id(policy_id).get(\"name\")\n            if policy_id != self.get_dlt_policy().get(\"policy_id\"):\n                common.print_warning(\"WARNING: Incorrect Policy\", \n                                     f\"Expected the policy to be set to \\\"{ClustersHelper.POLICY_DLT_ONLY}\\\", found \\\"{policy_name}\\\".\")\n        return True\n\n    suite.test(test_function=test_cluster_policy, actual_value=None, \n               description=f\"The cluster policy should be <b>\\\"{ClustersHelper.POLICY_DLT_ONLY}\\\"</b>.\")\n    suite.test_equals(lambda: spec.get(\"clusters\")[0].get(\"num_workers\"), 0, \n                      description=f\"The number of spark workers should be <b>0</b>.\", hint=f\"Found [[ACTUAL_VALUE]] workers.\")\n\n    # validate pipeline development mode, current channel, pipeline triggered mode\n    suite.test_true(lambda: spec.get(\"development\") != self.is_smoke_test(), \n                    description=f\"The pipeline mode should be set to \\\"<b>Development</b>\\\".\")\n    suite.test(test_function=lambda: {spec.get(\"channel\") is None or spec.get(\"channel\").upper() == \"CURRENT\"}, \n               actual_value=spec.get(\"channel\"), \n               description=f\"The channel should be set to \\\"<b>Current</b>\\\".\", hint=f\"Found \\\"<b>[[ACTUAL_VALUE]]</b>\\\"\")\n    suite.test_false(lambda: spec.get(\"continuous\"), \n                     description=f\"Expected the Pipeline mode to be \\\"<b>Triggered</b>\\\".\", \n                     hint=f\"Found \\\"<b>Continuous</b>\\\".\")\n\n    # validate photon enabled\n    # suite.test_true(lambda: spec.get(\"photon\"), description=f\"Photon should be enabled.\")                     \n\n    if display: suite.display_results()\n    assert suite.passed, \"One or more tests failed; please double check your work.\"  \n\nNone   \n","commandVersion":1,"state":"finished","results":null,"resultDbfsStatus":"INLINED_IN_TREE","resultDbfsErrorMessage":null,"errorSummary":null,"errorTraceType":null,"error":null,"errorDetails":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"useConsistentColors":false,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"isLockedInExamMode":false,"iPythonMetadata":null,"metadata":{},"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"tableResultIndex":null,"listResultMetadata":[],"subcommandOptions":null,"contentSha256Hex":null,"nuid":"03c50582-71fb-47cf-aa05-edc80b366332"}],"dashboards":[],"guid":"db58aa93-f513-4d05-9d9f-a8f24a8ed949","globalVars":{},"iPythonMetadata":null,"inputWidgets":{},"notebookMetadata":{},"reposExportFormat":"SOURCE"}